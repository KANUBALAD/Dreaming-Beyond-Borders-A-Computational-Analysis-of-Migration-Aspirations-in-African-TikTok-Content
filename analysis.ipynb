{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f9529d9",
   "metadata": {},
   "source": [
    "# Analysis of TikTok data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7af085",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13614284",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data_csv/old_tiktok_videos_detailed.csv\")\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b25c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633e977",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Load your data\n",
    "df = pd.read_csv('data_csv/old_tiktok_videos_detailed.csv')  # Replace with your file path\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove URLs, mentions, and hashtags (but we'll extract hashtags separately)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)  # Remove # but keep the word\n",
    "    \n",
    "    # Remove punctuation and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning to captions\n",
    "df['cleaned_caption'] = df['caption'].apply(clean_text)\n",
    "\n",
    "# Create tokens for LDA\n",
    "texts = [text.split() for text in df['cleaned_caption']]\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# Perform LDA\n",
    "num_topics = 5  # You can adjust this based on your data\n",
    "lda_model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)\n",
    "\n",
    "# Display topics\n",
    "print(\"LDA Topics:\")\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f\"Topic {idx}: {topic}\")\n",
    "\n",
    "# Visualize topics (optional but helpful)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.display(vis)\n",
    "\n",
    "# Supplementary method: N-gram analysis\n",
    "def get_ngrams(text, n=2):\n",
    "    tokens = word_tokenize(text)\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "# Generate and display most common bigrams and trigrams\n",
    "all_text = ' '.join(df['cleaned_caption'].tolist())\n",
    "bigrams = get_ngrams(all_text, 2)\n",
    "trigrams = get_ngrams(all_text, 3)\n",
    "\n",
    "print(\"\\nMost common bigrams:\")\n",
    "print(Counter(bigrams).most_common(10))\n",
    "\n",
    "print(\"\\nMost common trigrams:\")\n",
    "print(Counter(trigrams).most_common(10))\n",
    "\n",
    "# Generate a word cloud for visual exploration\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of TikTok Captions')\n",
    "plt.show()\n",
    "\n",
    "# Key Word in Context (KWIC) function\n",
    "def kwic(text, keyword, window=5):\n",
    "    \"\"\"\n",
    "    Extract keyword in context from text\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text)\n",
    "    matches = []\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == keyword:\n",
    "            start = max(0, i - window)\n",
    "            end = min(len(tokens), i + window + 1)\n",
    "            context = ' '.join(tokens[start:end])\n",
    "            matches.append(context)\n",
    "    \n",
    "    return matches\n",
    "\n",
    "# Example: Find contexts for key migration-related terms\n",
    "migration_terms = ['uk', 'visa', 'work', 'life', 'nurse', 'ghana']\n",
    "for term in migration_terms:\n",
    "    contexts = []\n",
    "    for caption in df['cleaned_caption']:\n",
    "        contexts.extend(kwic(caption, term))\n",
    "    \n",
    "    if contexts:\n",
    "        print(f\"\\nContexts for '{term}':\")\n",
    "        for i, context in enumerate(contexts[:3]):  # Show first 3 examples\n",
    "            print(f\"  {i+1}. {context}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93a477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import CoherenceModel\n",
    "# from textblob import TextBlob\n",
    "\n",
    "# Assuming you've already run the previous code and have these variables\n",
    "# df, lda_model, corpus, dictionary, texts\n",
    "\n",
    "# Step 1: Refine the topic model by finding the optimal number of topics\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=1):\n",
    "    \"\"\"\n",
    "    Compute coherence values for different numbers of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15, random_state=42)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "    \n",
    "    return model_list, coherence_values\n",
    "\n",
    "# Compute coherence for different numbers of topics\n",
    "limit = 10\n",
    "start = 2\n",
    "step = 1\n",
    "model_list, coherence_values = compute_coherence_values(dictionary, corpus, texts, limit, start, step)\n",
    "\n",
    "# Plot coherence scores\n",
    "x = range(start, limit, step)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Number of Topics\")\n",
    "plt.ylabel(\"Coherence Score\")\n",
    "plt.title(\"Coherence Scores by Number of Topics\")\n",
    "plt.show()\n",
    "\n",
    "# Select the model with the highest coherence score\n",
    "best_index = np.argmax(coherence_values)\n",
    "best_lda_model = model_list[best_index]\n",
    "optimal_num_topics = x[best_index]\n",
    "\n",
    "print(f\"Optimal number of topics: {optimal_num_topics}\")\n",
    "print(f\"Best coherence score: {coherence_values[best_index]:.4f}\")\n",
    "\n",
    "# Step 2: Assign descriptive labels to each topic\n",
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    \"\"\"\n",
    "    Assign topics to each document and create a dataframe\n",
    "    \"\"\"\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the dominant topic, percentage contribution, and keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic, 4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the dataframe\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return sent_topics_df\n",
    "\n",
    "# Create topic assignment dataframe\n",
    "df_topic_sents_keywords = format_topics_sentences(best_lda_model, corpus, df['cleaned_caption'])\n",
    "\n",
    "# Merge with original dataframe\n",
    "df_merged = pd.concat([df, df_topic_sents_keywords], axis=1)\n",
    "\n",
    "# Manually assign descriptive names based on topic keywords\n",
    "topic_names = {\n",
    "    0: \"Career Advancement & Nursing Process\",\n",
    "    1: \"Education & Exam Preparation\",\n",
    "    2: \"Financial Benefits & Opportunities\",\n",
    "    3: \"Daily Life & Cultural Experience\",\n",
    "    4: \"Nostalgia & Connection to Home\"\n",
    "}\n",
    "\n",
    "# Add topic names to dataframe\n",
    "df_merged['Topic_Name'] = df_merged['Dominant_Topic'].map(topic_names)\n",
    "\n",
    "# Step 3: Calculate topic prevalence\n",
    "topic_prevalence = df_merged['Topic_Name'].value_counts().reset_index()\n",
    "topic_prevalence.columns = ['Topic', 'Count']\n",
    "topic_prevalence['Percentage'] = (topic_prevalence['Count'] / topic_prevalence['Count'].sum()) * 100\n",
    "\n",
    "print(\"Topic Prevalence:\")\n",
    "print(topic_prevalence)\n",
    "\n",
    "# Visualize topic prevalence\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Percentage', y='Topic', data=topic_prevalence, palette='viridis')\n",
    "plt.title('Prevalence of Migration Narrative Topics')\n",
    "plt.xlabel('Percentage of Videos')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Cross-validate with qualitative analysis\n",
    "# Sample 5 captions from each topic for manual review\n",
    "for topic in df_merged['Topic_Name'].unique():\n",
    "    print(f\"\\n=== {topic} ===\")\n",
    "    sample_captions = df_merged[df_merged['Topic_Name'] == topic].sample(5, random_state=42)['caption']\n",
    "    for i, caption in enumerate(sample_captions):\n",
    "        print(f\"{i+1}. {caption}\")\n",
    "\n",
    "# Step 5: Connect topics to engagement metrics\n",
    "# First, calculate engagement rate (using likes + shares as proxy)\n",
    "df_merged['engagement'] = df_merged['likes'] + df_merged['shares']\n",
    "\n",
    "# Calculate average engagement by topic\n",
    "topic_engagement = df_merged.groupby('Topic_Name')['engagement'].agg(['mean', 'std', 'count']).reset_index()\n",
    "topic_engagement.columns = ['Topic', 'Mean_Engagement', 'Std_Engagement', 'Count']\n",
    "\n",
    "print(\"\\nEngagement by Topic:\")\n",
    "print(topic_engagement)\n",
    "\n",
    "# Visualize engagement by topic\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Mean_Engagement', y='Topic', data=topic_engagement, palette='magma')\n",
    "plt.title('Average Engagement by Topic')\n",
    "plt.xlabel('Average Engagement (Likes + Shares)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Perform sentiment analysis for RQ2\n",
    "def get_sentiment(text):\n",
    "    \"\"\"\n",
    "    Get sentiment polarity using TextBlob\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return 0\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Apply sentiment analysis\n",
    "df_merged['sentiment'] = df_merged['cleaned_caption'].apply(get_sentiment)\n",
    "\n",
    "# Categorize sentiment\n",
    "def categorize_sentiment(score):\n",
    "    if score > 0.1:\n",
    "        return 'Positive'\n",
    "    elif score < -0.1:\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "df_merged['sentiment_label'] = df_merged['sentiment'].apply(categorize_sentiment)\n",
    "\n",
    "# Analyze sentiment by topic\n",
    "sentiment_by_topic = pd.crosstab(df_merged['Topic_Name'], df_merged['sentiment_label'], normalize='index') * 100\n",
    "print(\"\\nSentiment Distribution by Topic (%):\")\n",
    "print(sentiment_by_topic)\n",
    "\n",
    "# Visualize sentiment by topic\n",
    "sentiment_by_topic.plot(kind='bar', stacked=True, figsize=(12, 6), colormap='coolwarm')\n",
    "plt.title('Sentiment Distribution by Topic')\n",
    "plt.ylabel('Percentage')\n",
    "plt.xlabel('Topic')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Sentiment')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between sentiment and engagement\n",
    "correlation = df_merged['sentiment'].corr(df_merged['engagement'])\n",
    "print(f\"\\nCorrelation between sentiment and engagement: {correlation:.3f}\")\n",
    "\n",
    "# Step 7: Prepare for RQ3 (Interpretation of migration bias)\n",
    "# Calculate the proportion of positive vs negative/neutral content\n",
    "positive_content = len(df_merged[df_merged['sentiment_label'] == 'Positive']) / len(df_merged) * 100\n",
    "print(f\"\\nPercentage of videos with positive sentiment: {positive_content:.1f}%\")\n",
    "\n",
    "# Compare engagement for positive vs negative/neutral content\n",
    "sentiment_engagement = df_merged.groupby('sentiment_label')['engagement'].mean()\n",
    "print(\"\\nAverage engagement by sentiment:\")\n",
    "print(sentiment_engagement)\n",
    "\n",
    "# Save the enriched dataframe for further analysis\n",
    "df_merged.to_csv('tiktok_data_enriched.csv', index=False)\n",
    "print(\"\\nEnriched data saved to 'tiktok_data_enriched.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c083252f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2fde1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiktok_env (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
